<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>Veille technologique</title>
  <link rel="stylesheet" href="../CSS/style.css">
</head>
<body>
<header><h1>Veille technologique</h1></header>
<nav><a href="../index.html">Accueil</a></nav>
<main>
  <h2>ThÃ©matique</h2>
  <p>jâ€™ai choisi de traiter le sujet sur lâ€™ia plus particulierement les chatbots, 
    <br>etant moi meme un consomateur de lâ€™application character ai, le sujet ne mâ€™etais pas inconnu, au cours de lâ€™annÃ©e 2024-2025 jâ€™ai traitÃ© plusieurs sujets sur les chatbots, 
    <br>de chatbot contre les violences sexuelles a la ressurection du plus vieux chatbot de lâ€™histoire en passant par lâ€™adaptation du style dâ€™ecriture de Claude ou la rÃ©bÃ©llion de Gemini, ou encore les chatbots polÃ©miques, a lâ€™effigie de jÃ©sus ou dâ€™un certain dictateur allemand </p>
  <h2>Grok s'emmÃªle les pinceaux et rÃ©pand des fake news</h2>
  <p>Date : 15/12/2025</p>
  <p>ğŸ‘‰ Le chatbot IA Grok, dÃ©veloppÃ© par xAI dâ€™Elon Musk, a diffusÃ© de nombreuses informations erronÃ©es au sujet de la rÃ©cente fusillade de Bondi Beach en Australie. 

<br>
ğŸ§  Ce que Grok a mal fait

Grok a fausssement identifiÃ© le tÃ©moin hÃ©roÃ¯que Ahmed al Ahmed (qui a neutralisÃ© un tireur) en lui attribuant plusieurs identitÃ©s incorrectes, par exemple :

le prÃ©sentant comme un Â« otage israÃ©lien dÃ©tenu par le Hamas Â» ;

ou comme un autre homme fictif nommÃ© Edward Crabtree. 
<br>

Dans certaines rÃ©ponses, Grok a mÃªme affirmÃ© que des vidÃ©os du drame montraient autre chose, par exemple :

un incident ancien (un homme grimpant dans un palmier) ;

ou une vidÃ©o dâ€™un autre Ã©vÃ©nement (le Cyclone Alfred en Australie). 


Lâ€™IA a parfois mÃ©langÃ© des informations sans rapport, comme des commentaires sur des conflits internationaux, qui nâ€™avaient rien Ã  voir avec la fusillade Ã  Bondi. 
<br>
âš ï¸ Pourquoi ces erreurs ont eu lieu

Grok rÃ©pond automatiquement Ã  des questions dâ€™utilisateurs sur X en se basant sur des contenus trÃ¨s rÃ©cents ou peu fiables qui circulent sur Internet (incluant des sites gÃ©nÃ©rÃ©s par IA ou des posts viraux). 
Startup Daily

Cela montre les limites des chatbots dâ€™IA pour les Ã©vÃ©nements rÃ©cents et sensibles, notamment lorsquâ€™ils manquent de sources fiables ou vÃ©rifiÃ©es. 
<br>

ğŸ§© Corrections partielles

AprÃ¨s avoir Ã©tÃ© mis en Ã©vidence, certaines rÃ©ponses de Grok ont Ã©tÃ© modifiÃ©es ou corrigÃ©es par lâ€™IA elle-mÃªme dans des interactions ultÃ©rieures.</p>
  <h2>Un chatbot un peu trop bavard</h2>
  <p>Date : 19/11/2025</p>
  <p>ğŸ§  Contexte
<br>
Un chatbot alimentÃ© par lâ€™intelligence artificielle, utilisÃ© dans un contexte professionnel, a rÃ©vÃ©lÃ© des secrets internes dâ€™une entreprise, ce qui soulÃ¨ve de fortes inquiÃ©tudes sur la gestion, la sÃ©curitÃ© et la maÃ®trise de ces outils numÃ©riques.
<br>
ğŸ“Œ Ce qui sâ€™est passÃ©
<br>
Le chatbot IA, pourtant destinÃ© Ã  aider ou assister dans un cadre contrÃ´lÃ©, a communiquÃ© des informations confidentielles ou sensibles Ã  des personnes qui nâ€™Ã©taient pas censÃ©es y avoir accÃ¨s.

Cet incident illustre un usage mal encadrÃ© ou prÃ©maturÃ© de lâ€™IA en entreprise : sans rÃ¨gles claires, lâ€™outil ne sait pas distinguer ce qui peut ou non Ãªtre divulguÃ©.
<br>
âš ï¸ Pourquoi câ€™est problÃ©matique
<br>
Les chatbots produisent leurs rÃ©ponses en sâ€™appuyant sur des donnÃ©es existantes et des probabilitÃ©s. Sans formation adaptÃ©e et limites strictes, ils peuvent inventer des informations ou rÃ©vÃ©ler du contenu inappropriÃ© ou confidentiel.

Cette affaire montre lâ€™importance de configurer, restreindre, tester et surveiller les systÃ¨mes dâ€™IA avant de les dÃ©ployer dans un environnement contenant des donnÃ©es internes.
<br>
ğŸ› ï¸ Ce que les experts recommandent
<br>
Les entreprises doivent Â« Ã©duquer Â» leurs chatbots, en dÃ©finissant prÃ©cisÃ©ment ce quâ€™ils ont le droit de dire ou non.

Une supervision humaine, des droits dâ€™accÃ¨s limitÃ©s et des mesures de sÃ©curitÃ© techniques sont indispensables pour Ã©viter toute fuite dâ€™informations sensibles.
<br>
ğŸ“Œ En rÃ©sumÃ© : cet incident rappelle que les chatbots IA peuvent Ãªtre utiles, mais quâ€™ils reprÃ©sentent un risque rÃ©el pour la confidentialitÃ© sâ€™ils sont mal paramÃ©trÃ©s ou utilisÃ©s sans garde-fous.</p>
</main>
<footer><p>Portfolio BTS SIO</p></footer>
</body>
</html>
